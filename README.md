# ğŸ¢ ROS 2 DRL Training System for TurtleBot Navigation

## ğŸ“Œ Project Overview

This project provides a complete, **containerized foundation** for training a **Deep Reinforcement Learning (DRL)** agent to achieve **autonomous navigation** (collision avoidance and goal seeking) in a simulated environment.

It features a dedicated **Trainer Node** within a ROS 2 Foxy package (`turtlebot_nav`) that acts as the essential bridge â€” translating raw **LiDAR** and **Odometry** data into a standardized **state space** for an RL agent, and translating the agent's actions back into executable **velocity commands**.

The entire environment is **containerized with Docker** and uses **X11 forwarding** to seamlessly display the Gazebo simulation GUI on the host machine.

---

## ğŸ§  System Architecture Overview

This project implements the core **Agentâ€“Environment loop** for Reinforcement Learning within the ROS 2 framework:

<p align="center">
  <img src="licensed-image.jpeg" alt="Reinforcement Learning Feedback Loop" width="600"/>
</p>

| Component             | Technology      | Role                                                                                           |
| --------------------- | --------------- | ---------------------------------------------------------------------------------------------- |
| **Gazebo Simulation** | Gazebo, ROS 2   | The virtual "world" (environment). Handles physics, LiDAR sensor, and robot dynamics.          |
| **Robot Model**       | XACRO / URDF    | Defines the differential-drive TurtleBot-like robot and its sensor placement.                  |
| **Trainer Node**      | ROS 2 (`rclpy`) | Core RL interface. Subscribes to sensors, calculates rewards, and publishes velocity commands. |
| **RL Agent**          | Python (stub)   | Placeholder for the deep learning model. Selects optimal action based on current state.        |

---

## âš™ï¸ Detailed Workflow â€“ Stage by Stage

### 1. ğŸ¤– Robot & World Definition

**Files:** `urdf/robot.urdf.xacro`, `worlds/small.world`

* The XACRO file defines a **TurtleBot-like robot**, including its geometry and inertia.
* It embeds Gazebo plugins for:

  * **Differential Drive:** listens to `/cmd_vel`
  * **360Â° LiDAR:** publishes to `/gazebo_ros_laser_controller/out`
* The `small.world` file defines a simple arena with boundary walls and box obstacles.

ğŸ’¡ The environment is intentionally simple to help the RL agent quickly learn basic collision avoidance.

---

### 2. ğŸš€ Launch Orchestration

**File:** `launch/sim.launch.py`

This ROS 2 launch file manages the entire simulation startup sequence using timed actions:

* Starts the Gazebo **physics server (`gzserver`)** and **GUI client (`gzclient`)**
* Starts `robot_state_publisher` to process the XACRO robot description
* **3.0s delay:** Spawns the robot into the Gazebo world
* **5.0s delay:** Starts the main Trainer Node after all topics are active

---

### 3. ğŸ“Š Sensor Processing and State Space

**File:** `src/sensor_helpers.py`

This module handles the crucial task of converting raw ROS messages into RL-friendly state vectors:

* `process_scan`: Converts the 360Â° `LaserScan` message into a normalized (0â€“1) 1D NumPy array
* `pose_to_state`: Converts the `Odometry` message into a local pose array

ğŸ› ï¸ The agent only sees the **normalized state vector**, not the full ROS message structure.

---

### 4. ğŸ§  RL Training Logic (The Loop)

**Files:** `src/trainer_node.py`, `src/rl_agent.py`

#### ğŸŒ€ Trainer Node (`trainer_node.py`)

Runs at **10 Hz**, continuously performing the DRL cycle:

1. Read the latest processed sensor data (**state**).
2. *(Stub)* Compute the **reward** based on state and previous action.
3. *(Stub)* Send state to `RLAgent` to get the next **action**.
4. Execute the action by publishing to `/cmd_vel`.
5. Handle **episode termination** (collision or goal reached) by resetting the simulation.

#### ğŸ§  RL Agent (`rl_agent.py`)

A stub class ready for integration with TensorFlow or PyTorch. It defines essential methods:

* `select_action(state)` â€“ using an Îµ-greedy policy
* `train(...)` â€“ training logic placeholder

---

## ğŸ“‚ Project Structure

```
ros2_ws/
â””â”€â”€ src/
    â””â”€â”€ turtlebot_nav/
        â”œâ”€â”€ CMakeLists.txt              # Build instructions (ament_cmake)
        â”œâ”€â”€ package.xml                 # Package dependencies and metadata
        â”œâ”€â”€ requirements.txt            # Python dependencies (e.g., numpy)
        â”‚
        â”œâ”€â”€ launch/
        â”‚   â””â”€â”€ sim.launch.py           # ROS 2 launch file for starting Gazebo and nodes
        â”‚
        â”œâ”€â”€ src/
        â”‚   â”œâ”€â”€ trainer_node.py         # Main RL training loop (ROS 2 Node)
        â”‚   â”œâ”€â”€ rl_agent.py             # DRL Agent class stub (needs ML framework)
        â”‚   â””â”€â”€ sensor_helpers.py       # Utilities for processing LiDAR and Odometry data
        â”‚
        â”œâ”€â”€ urdf/
        â”‚   â””â”€â”€ robot.urdf.xacro        # TurtleBot-like robot description with Gazebo plugins
        â”‚
        â””â”€â”€ worlds/
            â””â”€â”€ small.world             # Gazebo simulation environment (6x6m arena)
```

---

## â–¶ï¸ How to Run the Training System

These steps assume you are running on a **Linux host (e.g., Ubuntu)** with **Docker installed**.

---

### ğŸ–¥ï¸ Pre-Requisite: X11 Forwarding Setup

Grant the Docker container permission to display the Gazebo GUI on your desktop:

```bash
xhost +local:docker
```

âœ… Output:

```
non-network local connections being added to access control list
```

---

### ğŸ³ Execution

#### 1. Build the Docker Image

From your project root (containing `ros2_ws` and `Dockerfile`):

```bash
docker build -t turtlebot_nav_foxy .
```

#### 2. Launch the System

Run the container with X11 forwarding and launch the ROS 2 simulation:

```bash
docker run -it --rm \
    --name rl_trainer \
    --net=host \
    --privileged \
    -e DISPLAY=$DISPLAY \
    -v /tmp/.X11-unix:/tmp/.X11-unix:rw \
    turtlebot_nav_foxy \
    bash -c 'source /opt/ros/foxy/setup.bash && source /ros2_ws/install/setup.bash && ros2 launch turtlebot_nav sim.launch.py'
```

---

### âœ… Expected Output

* The **Gazebo GUI** opens on your host, showing the `small.world` arena and TurtleBot model.
* Terminal logs from the Trainer Node confirm the **anti-collision routine** is active and running.

---

## ğŸ“Š What This Demonstrates

This setup shows the **fundamental loop** behind any simulation-to-real-world robotics deployment:

| Stage          | What Happens                                  | Real-World Equivalent                        |
| -------------- | --------------------------------------------- | -------------------------------------------- |
| ğŸŒ Environment | Gazebo physics + LiDAR data generation        | Physical world interaction & sensor hardware |
| âš™ï¸ Interface   | ROS 2 node handles subscriptions/publications | Robotâ€™s onboard controller/microcontroller   |
| ğŸ§  Agent       | DRL model selects actions                     | Embedded AI policy in autonomous robot       |
| ğŸ” Feedback    | Collisions trigger episode reset              | Sensor feedback loop for safe operation      |

---

## ğŸš€ Future Extensions

This system is a **runnable foundation**. Recommended next steps for full DRL functionality:

* ğŸ§  **Deep Learning Integration:** Implement a DRL network (e.g., DQN, PPO) in `src/rl_agent.py` using TensorFlow or PyTorch.
* ğŸ† **Reward Function & Goal Tracking:** Add reward logic (positive for progress, negative for collisions) and include goal coordinates in the state.
* ğŸ“š **Experience Replay:** Add a replay buffer to stabilize off-policy learning algorithms like DQN.

---
